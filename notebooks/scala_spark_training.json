{"paragraphs":[{"text":"%md\n## Welcome to SKIL. \n\n### This notebook demonstrates how to run distributed training on Spark with SKIL, starting from a simple Keras model. You will learn the following:\n- Create a model with Keras to classify hand-written digits (MNIST dataset).\n- Import it into Deeplearning4j\n- Create a Spark model from the imported model\n- Define distributed training data\n- Run distributed model training \n\nWe'll walk you through step by step. You can just read through the notebook cells and execute the code linearly, following the instructions as you go. \n","user":"admin","dateUpdated":"2019-03-06T15:25:36+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Welcome to SKIL.</h2>\n<h3>This notebook demonstrates how to run distributed training on Spark with SKIL, starting from a simple Keras model. You will learn the following:</h3>\n<ul>\n<li>Create a model with Keras to classify hand-written digits (MNIST dataset).</li>\n<li>Import it into Deeplearning4j</li>\n<li>Create a Spark model from the imported model</li>\n<li>Define distributed training data</li>\n<li>Run distributed model training</li>\n</ul>\n<p>We'll walk you through step by step. You can just read through the notebook cells and execute the code linearly, following the instructions as you go.</p>\n"}]},"apps":[],"jobName":"paragraph_1551866297812_-1684828382","id":"20180911-080353_327652166","dateCreated":"2019-03-06T09:58:17+0000","dateStarted":"2019-03-06T15:25:36+0000","dateFinished":"2019-03-06T15:25:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2296"},{"text":"%md\n## Define a Keras model and save it","user":"admin","dateUpdated":"2019-03-06T15:26:56+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1551885991823_1031718996","id":"20190306-152631_1669965717","dateCreated":"2019-03-06T15:26:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2711","dateFinished":"2019-03-06T15:26:56+0000","dateStarted":"2019-03-06T15:26:56+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Define a Keras model and save it</h2>\n"}]}},{"text":"%pyspark\n\nimport numpy as np\nimport tensorflow as tf\n\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.regularizers import l2\nfrom keras.optimizers import Adam\n\n# The model hyperparameters are the same as for the DL4J model earlier.\nbatch_size = 256\nnum_output = 10\nepochs = 5\nrng_seed = 42\n\n# We fix the random seed for reproducibility\nnp.random.seed(42)\ntf.set_random_seed(42)\n\n# Load and reshape MNIST data\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train.reshape(60000, 784)\nx_test = x_test.reshape(10000, 784)\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\ny_train = keras.utils.to_categorical(y_train, num_output)\ny_test = keras.utils.to_categorical(y_test, num_output)\n\n# In Keras you add layers to a Sequential model and compile it.\nmodel = Sequential()\nmodel.add(Dense(768, activation='relu', input_shape=(784,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(num_output, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n  optimizer=Adam(lr=0.004, amsgrad=True),\n  metrics=['accuracy'])\n\nmodel.save('/tmp/model.h5')\n","user":"admin","dateUpdated":"2019-03-06T15:26:45+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Using TensorFlow backend.\nDownloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n\n   16384/11490434 [..............................] - ETA: 0s\n   24576/11490434 [..............................] - ETA: 44s\n   57344/11490434 [..............................] - ETA: 42s\n  139264/11490434 [..............................] - ETA: 25s\n  147456/11490434 [..............................] - ETA: 39s\n  507904/11490434 [>.............................] - ETA: 13s\n  802816/11490434 [=>............................] - ETA: 8s \n  860160/11490434 [=>............................] - ETA: 8s\n 1204224/11490434 [==>...........................] - ETA: 8s\n 2531328/11490434 [=====>........................] - ETA: 3s\n 2834432/11490434 [======>.......................] - ETA: 3s\n 3047424/11490434 [======>.......................] - ETA: 3s\n 3833856/11490434 [=========>....................] - ETA: 2s\n 4128768/11490434 [=========>....................] - ETA: 2s\n 4374528/11490434 [==========>...................] - ETA: 2s\n 4653056/11490434 [===========>..................] - ETA: 2s\n 4931584/11490434 [===========>..................] - ETA: 1s\n 5210112/11490434 [============>.................] - ETA: 1s\n 5505024/11490434 [=============>................] - ETA: 1s\n 5750784/11490434 [==============>...............] - ETA: 1s\n 6176768/11490434 [===============>..............] - ETA: 1s\n 6324224/11490434 [===============>..............] - ETA: 1s\n 6766592/11490434 [================>.............] - ETA: 1s\n 6897664/11490434 [=================>............] - ETA: 1s\n 7307264/11490434 [==================>...........] - ETA: 1s\n 7487488/11490434 [==================>...........] - ETA: 1s\n 7864320/11490434 [===================>..........] - ETA: 0s\n 8060928/11490434 [====================>.........] - ETA: 0s\n 8404992/11490434 [====================>.........] - ETA: 0s\n 8552448/11490434 [=====================>........] - ETA: 0s\n 8830976/11490434 [======================>.......] - ETA: 0s\n 9207808/11490434 [=======================>......] - ETA: 0s\n 9437184/11490434 [=======================>......] - ETA: 0s\n 9551872/11490434 [=======================>......] - ETA: 0s\n10387456/11490434 [==========================>...] - ETA: 0s\n10518528/11490434 [==========================>...] - ETA: 0s\n10944512/11490434 [===========================>..] - ETA: 0s\n11370496/11490434 [============================>.] - ETA: 0s\n11493376/11490434 [==============================] - 3s 0us/step\n\n11501568/11490434 [==============================] - 3s 0us/step\n2019-03-06 15:11:42.961684: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2019-03-06 15:11:43.496568: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"}]},"apps":[],"jobName":"paragraph_1551866297821_-1688291122","id":"20180911-082444_508775148","dateCreated":"2019-03-06T09:58:17+0000","dateStarted":"2019-03-06T15:11:21+0000","dateFinished":"2019-03-06T15:11:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2297"},{"text":"%md\n\n## Load the Keras model into DL4J","user":"admin","dateUpdated":"2019-03-06T15:27:27+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1551886027069_301342990","id":"20190306-152707_865975382","dateCreated":"2019-03-06T15:27:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2787","dateFinished":"2019-03-06T15:27:27+0000","dateStarted":"2019-03-06T15:27:27+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Load the Keras model into DL4J</h2>\n"}]}},{"text":"import org.deeplearning4j.nn.modelimport.keras.KerasModelImport;\nval model = KerasModelImport.importKerasSequentialModelAndWeights(\"/tmp/model.h5\");","user":"admin","dateUpdated":"2019-03-06T15:15:28+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.deeplearning4j.nn.modelimport.keras.KerasModelImport\nmodel: org.deeplearning4j.nn.multilayer.MultiLayerNetwork = org.deeplearning4j.nn.multilayer.MultiLayerNetwork@1748e423\n"}]},"apps":[],"jobName":"paragraph_1551884167443_-890672044","id":"20190306-145607_1981771438","dateCreated":"2019-03-06T14:56:07+0000","dateStarted":"2019-03-06T15:15:28+0000","dateFinished":"2019-03-06T15:15:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2298"},{"user":"admin","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1551886062151_-53148665","id":"20190306-152742_1390242155","dateCreated":"2019-03-06T15:27:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2936","text":"%md\n\n## Define an RDD of training data","dateUpdated":"2019-03-06T15:27:58+0000","dateFinished":"2019-03-06T15:27:58+0000","dateStarted":"2019-03-06T15:27:58+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Define an RDD of training data</h2>\n"}]}},{"text":"import org.nd4j.linalg.dataset.DataSet;\nimport scala.collection.mutable.ArrayBuffer;\n\nval batchSizePerWorker = 32;\nval iterTrain = new MnistDataSetIterator(batchSizePerWorker, true, 12345);\nval iterTest = new MnistDataSetIterator(batchSizePerWorker, false, 12345);\nval trainDataList = new ArrayBuffer[DataSet]();\nval testDataList = new ArrayBuffer[DataSet]();\nwhile (iterTrain.hasNext()) {\n    trainDataList += iterTrain.next();\n}\nwhile (iterTest.hasNext()) {\n    testDataList += iterTest.next();\n}\n\nval trainData = sc.parallelize(trainDataList);\nval testData = sc.parallelize(testDataList);","user":"admin","dateUpdated":"2019-03-06T15:20:50+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.nd4j.linalg.dataset.DataSet\nimport scala.collection.mutable.ArrayBuffer\nbatchSizePerWorker: Int = 32\niterTrain: org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator = org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator@39714982\niterTest: org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator = org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator@7a405a8f\ntrainDataList: scala.collection.mutable.ArrayBuffer[org.nd4j.linalg.dataset.DataSet] = ArrayBuffer()\ntestDataList: scala.collection.mutable.ArrayBuffer[org.nd4j.linalg.dataset.DataSet] = ArrayBuffer()\ntrainData: org.apache.spark.rdd.RDD[org.nd4j.linalg.dataset.DataSet] = ParallelCollectionRDD[2] at parallelize at <console>:144\ntestData: org.apache.spark.rdd.RDD[org.nd4j.linalg.dataset.DataSet] = ParallelCollectionRDD[3] at parallelize at <console>:143\n"}]},"apps":[],"jobName":"paragraph_1551883268551_225242094","id":"20190306-144108_739525887","dateCreated":"2019-03-06T14:41:08+0000","dateStarted":"2019-03-06T15:20:50+0000","dateFinished":"2019-03-06T15:20:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2299"},{"user":"admin","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1551886056036_688815781","id":"20190306-152736_1002680804","dateCreated":"2019-03-06T15:27:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2864","text":"%md\n\n## Wrap the imported model into a DL4J Spark model and train","dateUpdated":"2019-03-06T15:28:26+0000","dateFinished":"2019-03-06T15:28:26+0000","dateStarted":"2019-03-06T15:28:26+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Wrap the imported model into a DL4J Spark model and train</h2>\n"}]}},{"text":"import org.deeplearning4j.spark.api.TrainingMaster\nimport org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer\nimport org.deeplearning4j.spark.parameterserver.training.SharedTrainingMaster\nimport org.nd4j.parameterserver.distributed.conf.VoidConfiguration\nimport org.deeplearning4j.spark.api.RDDTrainingApproach;\n\n\nval voidConfiguration = VoidConfiguration.builder()\n    .unicastPort(40123).networkMask(\"10.0.0.0/16\").controllerAddress(\"127.0.0.1\").build();\n\nval tm = new SharedTrainingMaster.Builder(voidConfiguration, batchSizePerWorker)\n    .updatesThreshold(1e-3)\n    .rddTrainingApproach(RDDTrainingApproach.Direct)\n    .batchSizePerWorker(batchSizePerWorker)\n    .workersPerNode(4)\n    .build()\n\n//Create the Spark network\nval sparkNet = new SparkDl4jMultiLayer(sc, model, tm)\n\n//Execute training:\nval numEpochs = 10\nfor (z <- 0 until numEpochs) {\n    sparkNet.fit(trainData);\n}","user":"admin","dateUpdated":"2019-03-06T15:21:40+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"import org.deeplearning4j.spark.api.TrainingMaster\nimport org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer\nimport org.deeplearning4j.spark.parameterserver.training.SharedTrainingMaster\nimport org.nd4j.parameterserver.distributed.conf.VoidConfiguration\nimport org.deeplearning4j.spark.api.RDDTrainingApproach\nvoidConfiguration: org.nd4j.parameterserver.distributed.conf.VoidConfiguration = VoidConfiguration(streamId=119, unicastControllerPort=49876, multicastPort=59876, numberOfShards=1, faultToleranceStrategy=NONE, executionMode=SHARDED, shardAddresses=[], backupAddresses=[], transportType=ROUTED_UDP, meshBuildMode=PLAIN, networkMask=10.0.0.0/16, multicastNetwork=224.0.1.1, multicastInterface=null, ttl=4, forcedRole=null, useHS=false, useNS=false, retransmitTimeout=1000, responseTimeframe=500, responseTimeout=30000, chunksBufferSize=1073741824, maxChunkSize=65536, maxFailuresPerNode=3, controllerAddress=127.0.0.1, portSupplier=StaticPortSupplier(port=49876))\nwarning: there were 1 deprecation warning(s); re-run with -deprecation for details\ntm: org.deeplearning4j.spark.parameterserver.training.SharedTrainingMaster = SharedTrainingMaster(trainingHooks=null, voidConfiguration=VoidConfiguration(streamId=119, unicastControllerPort=49876, multicastPort=59876, numberOfShards=1, faultToleranceStrategy=NONE, executionMode=MANAGED, shardAddresses=[], backupAddresses=[], transportType=ROUTED_UDP, meshBuildMode=PLAIN, networkMask=10.0.0.0/16, multicastNetwork=224.0.1.1, multicastInterface=null, ttl=4, forcedRole=null, useHS=false, useNS=false, retransmitTimeout=1000, responseTimeframe=500, responseTimeout=30000, chunksBufferSize=1073741824, maxChunkSize=65536, maxFailuresPerNode=3, controllerAddress=127.0.0.1, portSupplier=StaticPortSupplier(port=49876)), numWorkers=null, numWorkersPerNode=4, workerPrefetchBatches=2, rddTrainingAppro...sparkNet: org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer = org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer@62673eaf\nnumEpochs: Int = 10\norg.apache.spark.SparkException: Job 1 cancelled part of cancelled job group zeppelin-20190306-144808_1222392859\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1370)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1619)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1157)\n\tat org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:440)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:46)\n\tat org.deeplearning4j.spark.parameterserver.training.SharedTrainingMaster.getTotalDataSetObjectCount(SharedTrainingMaster.java:330)\n\tat org.deeplearning4j.spark.parameterserver.training.SharedTrainingMaster.executeTrainingDirect(SharedTrainingMaster.java:348)\n\tat org.deeplearning4j.spark.parameterserver.training.SharedTrainingMaster.executeTraining(SharedTrainingMaster.java:563)\n\tat org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer.fit(SparkDl4jMultiLayer.java:261)\n\tat org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer.fit(SparkDl4jMultiLayer.java:248)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$f1c1a199038c8d219dafd99bcfc0b$$$$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:164)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$f1c1a199038c8d219dafd99bcfc0b$$$$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:163)\n\tat scala.collection.immutable.Range.foreach(Range.scala:141)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$33d793dde4292884a4720419646f1$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:163)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$33d793dde4292884a4720419646f1$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:170)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$33d793dde4292884a4720419646f1$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:172)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$33d793dde4292884a4720419646f1$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:174)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$725d9ae18728ec9520b65ad133e3b55$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:176)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$725d9ae18728ec9520b65ad133e3b55$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:178)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$725d9ae18728ec9520b65ad133e3b55$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:180)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$725d9ae18728ec9520b65ad133e3b55$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:182)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$725d9ae18728ec9520b65ad133e3b55$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:184)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$3d99ae6e19b65c7f617b22f29b431fb$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:186)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$3d99ae6e19b65c7f617b22f29b431fb$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:188)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$3d99ae6e19b65c7f617b22f29b431fb$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:190)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$3d99ae6e19b65c7f617b22f29b431fb$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:192)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$3d99ae6e19b65c7f617b22f29b431fb$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:194)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$ad149dbdbd963d0c9dc9b1d6f07f5e$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:196)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$ad149dbdbd963d0c9dc9b1d6f07f5e$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:198)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$ad149dbdbd963d0c9dc9b1d6f07f5e$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:200)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$ad149dbdbd963d0c9dc9b1d6f07f5e$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:202)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$ad149dbdbd963d0c9dc9b1d6f07f5e$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:204)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$6e49527b15a75f3b188beeb1837a4f1$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:206)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$6e49527b15a75f3b188beeb1837a4f1$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:208)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$6e49527b15a75f3b188beeb1837a4f1$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:210)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$6e49527b15a75f3b188beeb1837a4f1$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:212)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$6e49527b15a75f3b188beeb1837a4f1$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:214)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$93297bcd59dca476dd569cf51abed168$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:216)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$93297bcd59dca476dd569cf51abed168$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:218)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$93297bcd59dca476dd569cf51abed168$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:220)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$93297bcd59dca476dd569cf51abed168$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:222)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$$$93297bcd59dca476dd569cf51abed168$$$$$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:224)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:226)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:228)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:230)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:232)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:234)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:236)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:238)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:240)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:242)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:244)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:246)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:248)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:250)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:252)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:254)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:256)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:258)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:260)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:262)\n\tat $iwC$$iwC$$iwC$$iwC.<init>(<console>:264)\n\tat $iwC$$iwC$$iwC.<init>(<console>:266)\n\tat $iwC$$iwC.<init>(<console>:268)\n\tat $iwC.<init>(<console>:270)\n\tat <init>(<console>:272)\n\tat .<init>(<console>:276)\n\tat .<clinit>(<console>)\n\tat .<init>(<console>:7)\n\tat .<clinit>(<console>)\n\tat $print(<console>)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.zeppelin.spark.Utils.invokeMethod(Utils.java:38)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:1000)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:1205)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:1172)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:1165)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:97)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:498)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:175)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n"}]},"apps":[],"jobName":"paragraph_1551883688772_1154960713","id":"20190306-144808_1222392859","dateCreated":"2019-03-06T14:48:08+0000","dateStarted":"2019-03-06T15:21:40+0000","dateFinished":"2019-03-06T15:22:15+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:2300"},{"text":"\n","dateUpdated":"2019-03-06T09:58:17+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1551866297823_-1687521624","id":"20180911-091229_801908387","dateCreated":"2019-03-06T09:58:17+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2301"}],"name":"spark_training","id":"2E61H1MAG","angularObjects":{"2E4NCM6S6:existing_process":[],"2E4DA4XDX:existing_process":[],"2E65WEX5T:existing_process":[],"2E4DYPA48:existing_process":[],"2E6BW9UPD:existing_process":[],"2E7QBFJCU:existing_process":[],"2E67GM3CR:existing_process":[],"2E5M6R4AN:existing_process":[],"2E61T4SZM:existing_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}